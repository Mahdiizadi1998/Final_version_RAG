"""
Gradio Web UI for Advanced Multi-Modal RAG System
Easy-to-use interface for document upload and querying
"""

import gradio as gr
import os
from pathlib import Path
from typing import List, Dict, Any, Tuple
import time
from datetime import datetime

# Import RAG system components
from document_parser import AdvancedDocumentParser
from vision_processor import VisionProcessor
from metadata_extractor import UniversalGeothermalMetadataExtractor
from semantic_chunker import UltimateSemanticChunker
from raptor_tree import RAPTORTree
from hybrid_store import HybridIndexStore
from sql_store import SQLStore
from ingestion_pipeline import DocumentIngestionPipeline
from query_router import QueryRouter
from answer_generator import AnswerGenerator
from agentic_rag import AdvancedAgenticRAG
from ollama_client import test_ollama_connection


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GLOBAL STATE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GlobalState:
    """Singleton to hold initialized components"""
    def __init__(self):
        self.initialized = False
        self.rag_system = None
        self.pipeline = None
        self.upload_directory = "./uploaded_documents"
        self.error_message = None
        
    def initialize(self):
        """Initialize all RAG components"""
        if self.initialized:
            return True, "System already initialized"
        
        try:
            # Test Ollama connection first
            print("ğŸ”„ Testing Ollama connection...")
            if not test_ollama_connection():
                self.error_message = "âŒ Ollama server not responding. Please start Ollama first."
                return False, self.error_message
            
            print("âœ… Ollama connected successfully")
            
            # Create upload directory
            os.makedirs(self.upload_directory, exist_ok=True)
            
            # Initialize components
            print("ğŸ”„ Initializing RAG components...")
            
            parser = AdvancedDocumentParser()
            vision_proc = VisionProcessor()
            metadata_extractor = UniversalGeothermalMetadataExtractor()
            chunker = UltimateSemanticChunker()
            raptor = RAPTORTree()
            hybrid_store = HybridIndexStore()
            sql_store = SQLStore(":memory:")
            
            pipeline = DocumentIngestionPipeline(
                parser=parser,
                vision_processor=vision_proc,
                metadata_extractor=metadata_extractor,
                chunker=chunker,
                raptor=raptor,
                hybrid_store=hybrid_store,
                sql_store=sql_store
            )
            
            router = QueryRouter()
            answer_gen = AnswerGenerator()
            
            rag_system = AdvancedAgenticRAG(
                pipeline=pipeline,
                query_router=router,
                answer_generator=answer_gen
            )
            
            self.pipeline = pipeline
            self.rag_system = rag_system
            self.initialized = True
            
            print("âœ… All components initialized successfully")
            return True, "âœ… System initialized successfully"
            
        except Exception as e:
            self.error_message = f"âŒ Initialization error: {str(e)}"
            print(self.error_message)
            return False, self.error_message


# Create global state instance
state = GlobalState()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UPLOAD TAB FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_uploaded_files(files: List[Any]) -> Tuple[str, str]:
    """
    Process uploaded files through the RAG pipeline
    
    Args:
        files: List of uploaded file objects from Gradio
        
    Returns:
        Tuple of (status_message, stats_message)
    """
    if not state.initialized:
        success, msg = state.initialize()
        if not success:
            return msg, ""
    
    if not files:
        return "âš ï¸ No files uploaded", ""
    
    try:
        # Save uploaded files to directory
        saved_files = []
        for file in files:
            file_path = Path(file.name)
            destination = Path(state.upload_directory) / file_path.name
            
            # Copy file
            with open(file.name, 'rb') as src:
                with open(destination, 'wb') as dst:
                    dst.write(src.read())
            
            saved_files.append(destination)
        
        status_msg = f"ğŸ“ Saved {len(saved_files)} file(s)\n\n"
        
        # Process files through pipeline
        status_msg += "ğŸ”„ Processing documents through RAG pipeline...\n"
        status_msg += "   This may take a few minutes...\n\n"
        
        start_time = time.time()
        
        # Ingest the uploaded directory
        state.pipeline.ingest_directory(
            state.upload_directory,
            ['*.pdf', '*.docx', '*.xlsx']
        )
        
        elapsed = time.time() - start_time
        
        # Get statistics
        total_docs = len(state.pipeline.hybrid_store.documents)
        faiss_size = state.pipeline.hybrid_store.faiss_index.ntotal
        graph_nodes = state.pipeline.hybrid_store.graph.number_of_nodes()
        raptor_levels = len(state.pipeline.raptor.tree)
        
        status_msg += f"âœ… Processing complete in {elapsed:.1f} seconds\n\n"
        status_msg += f"ğŸ“Š Processed Files:\n"
        for f in saved_files:
            status_msg += f"   â€¢ {f.name}\n"
        
        stats_msg = f"""
ğŸ“Š **System Statistics**

**Documents Indexed:** {total_docs:,}
**FAISS Index Size:** {faiss_size:,}
**Graph Nodes:** {graph_nodes:,}
**RAPTOR Tree Levels:** {raptor_levels}

**Processing Time:** {elapsed:.1f}s
**Status:** Ready for queries âœ…
"""
        
        return status_msg, stats_msg
        
    except Exception as e:
        error_msg = f"âŒ Error processing files: {str(e)}"
        print(error_msg)
        return error_msg, ""


def clear_uploaded_files() -> Tuple[str, str]:
    """Clear all uploaded files and reset the system"""
    try:
        if os.path.exists(state.upload_directory):
            for file in Path(state.upload_directory).glob('*'):
                if file.is_file():
                    file.unlink()
        
        return "âœ… All files cleared", ""
    except Exception as e:
        return f"âŒ Error clearing files: {str(e)}", ""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QUERY TAB FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def query_system(question: str, show_sources: bool = True) -> Tuple[str, str]:
    """
    Query the RAG system with a question
    
    Args:
        question: User's question
        show_sources: Whether to display source documents
        
    Returns:
        Tuple of (answer, sources_info)
    """
    if not state.initialized:
        success, msg = state.initialize()
        if not success:
            return msg, ""
    
    if not question or question.strip() == "":
        return "âš ï¸ Please enter a question", ""
    
    # Check if documents are loaded
    if len(state.pipeline.hybrid_store.documents) == 0:
        return "âš ï¸ No documents loaded. Please upload documents first.", ""
    
    try:
        start_time = time.time()
        
        # Query the system
        result = state.rag_system.query(question, return_details=True)
        
        elapsed = time.time() - start_time
        
        # Format answer
        answer = result.get('answer', 'No answer generated')
        confidence = result.get('confidence', 0.0)
        is_grounded = result.get('is_grounded', False)
        query_type = result.get('query_type', 'Unknown')
        strategy = result.get('strategy', 'Unknown')
        
        answer_msg = f"""
## Answer

{answer}

---

**Confidence:** {confidence:.1%} {'âœ…' if confidence > 0.7 else 'âš ï¸'}  
**Grounded:** {'âœ… Yes' if is_grounded else 'âŒ No'}  
**Query Type:** {query_type}  
**Strategy:** {strategy}  
**Response Time:** {elapsed:.2f}s
"""
        
        # Format sources
        sources_msg = ""
        if show_sources and 'sources' in result:
            sources_msg = "\n## ğŸ“š Sources\n\n"
            for i, source in enumerate(result['sources'][:5], 1):
                doc_name = source.get('document', 'Unknown')
                page = source.get('page', 'N/A')
                well = source.get('well', 'N/A')
                snippet = source.get('snippet', '')[:200]
                
                sources_msg += f"**Source {i}:** {doc_name}\n"
                sources_msg += f"- Page: {page}\n"
                if well != 'N/A':
                    sources_msg += f"- Well: {well}\n"
                sources_msg += f"- Snippet: {snippet}...\n\n"
        
        return answer_msg, sources_msg
        
    except Exception as e:
        error_msg = f"âŒ Error querying system: {str(e)}"
        print(error_msg)
        return error_msg, ""


def get_example_questions() -> List[str]:
    """Return list of example questions"""
    return [
        "What is the temperature in well ADK-GT-01?",
        "Compare temperatures between all wells",
        "What formations are mentioned in the documents?",
        "Summarize all wells in the Slochteren Formation",
        "Which well has the highest temperature?",
        "What are the production rates for each well?",
        "List all pressure measurements",
        "What test types were performed?"
    ]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRADIO UI INTERFACE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_gradio_interface():
    """Create and configure the Gradio interface"""
    
    with gr.Blocks(
        title="Advanced RAG System for Geothermal Well Reports"
    ) as demo:
        
        gr.Markdown(
            """
            # ğŸŒ‹ Advanced Multi-Modal RAG System
            ## Geothermal Well Reports Analysis
            
            Upload your geothermal well documents and ask questions about temperatures, pressures, formations, and more!
            """
        )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # TAB 1: UPLOAD DOCUMENTS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        with gr.Tab("ğŸ“ Upload Documents"):
            gr.Markdown(
                """
                ### Upload Your Documents
                
                Supported formats: **PDF**, **DOCX**, **XLSX**
                
                The system will:
                - Extract text, tables, and images
                - Identify wells, formations, and technical data
                - Build searchable index with AI embeddings
                - Create knowledge graph for complex queries
                """
            )
            
            with gr.Row():
                with gr.Column(scale=2):
                    file_upload = gr.File(
                        label="Upload Documents",
                        file_count="multiple",
                        file_types=[".pdf", ".docx", ".xlsx"]
                    )
                    
                    with gr.Row():
                        submit_btn = gr.Button("ğŸ“¤ Process Documents", variant="primary", size="lg")
                        clear_btn = gr.Button("ğŸ—‘ï¸ Clear All", variant="secondary")
                    
                    upload_status = gr.Textbox(
                        label="Processing Status",
                        lines=10,
                        interactive=False
                    )
                
                with gr.Column(scale=1):
                    stats_display = gr.Markdown(
                        """
                        ğŸ“Š **System Statistics**
                        
                        No documents loaded yet.
                        
                        Upload documents to get started!
                        """
                    )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # TAB 2: ASK QUESTIONS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        with gr.Tab("â“ Ask Questions"):
            gr.Markdown(
                """
                ### Query Your Documents
                
                Ask questions about your geothermal well reports in natural language.
                
                The system uses:
                - **Hybrid Retrieval**: FAISS + BM25 + Knowledge Graph
                - **RAPTOR Tree**: Hierarchical summarization
                - **Intelligent Routing**: Optimizes strategy per query type
                - **Grounded Answers**: Citations from source documents
                """
            )
            
            with gr.Row():
                with gr.Column(scale=2):
                    question_input = gr.Textbox(
                        label="Your Question",
                        placeholder="e.g., What is the temperature in well ADK-GT-01?",
                        lines=3
                    )
                    
                    with gr.Row():
                        query_btn = gr.Button("ğŸ” Get Answer", variant="primary", size="lg")
                        show_sources_check = gr.Checkbox(
                            label="Show Sources",
                            value=True
                        )
                    
                    gr.Markdown("### ğŸ’¡ Example Questions")
                    example_questions = gr.Examples(
                        examples=get_example_questions(),
                        inputs=question_input
                    )
                
                with gr.Column(scale=2):
                    answer_output = gr.Markdown(label="Answer")
                    sources_output = gr.Markdown(label="Sources")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # TAB 3: SYSTEM INFO
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        with gr.Tab("â„¹ï¸ System Info"):
            gr.Markdown(
                """
                ## System Capabilities
                
                ### ğŸ¯ Key Features
                - **Multi-Modal Processing**: Text, tables, and technical images
                - **Triple Metadata Extraction**: Regex + NLP + LLM (7x faster)
                - **Late Chunking**: Better contextual embeddings
                - **Contextual Enrichment**: 49% improvement in retrieval
                - **RAPTOR Tree**: Hierarchical document summarization
                - **Knowledge Graph**: Relationship-based traversal
                
                ### ğŸ“Š Optimizations
                - Document-level metadata (7x faster than chunk-level)
                - Regex-based chunk detection (0.01s vs 4s with LLM)
                - Batch encoding (32 documents)
                - Table quality filtering
                - Duplicate column handling
                
                ### ğŸ”§ Technical Stack
                - **LLM**: Ollama (llama3.1:8b)
                - **Vision**: llava:7b
                - **Embeddings**: all-MiniLM-L6-v2 (384 dim)
                - **Vector DB**: FAISS
                - **Graph**: NetworkX
                - **UI**: Gradio
                
                ### ğŸ“ Supported Query Types
                1. **Factual**: Direct information retrieval
                2. **Comparison**: Multi-well analysis
                3. **Summary**: Formation/field overviews
                4. **Complex**: Multi-step reasoning
                5. **Exploratory**: Open-ended discovery
                
                ---
                
                **Version**: 1.0.0  
                **Last Updated**: November 2025  
                **Status**: âœ… Production Ready
                """
            )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # EVENT HANDLERS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Upload tab events
        submit_btn.click(
            fn=process_uploaded_files,
            inputs=[file_upload],
            outputs=[upload_status, stats_display]
        )
        
        clear_btn.click(
            fn=clear_uploaded_files,
            inputs=[],
            outputs=[upload_status, stats_display]
        )
        
        # Query tab events
        query_btn.click(
            fn=query_system,
            inputs=[question_input, show_sources_check],
            outputs=[answer_output, sources_output]
        )
        
        # Also allow Enter key to submit question
        question_input.submit(
            fn=query_system,
            inputs=[question_input, show_sources_check],
            outputs=[answer_output, sources_output]
        )
    
    return demo


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("="*70)
    print("ADVANCED MULTI-MODAL RAG SYSTEM - WEB UI")
    print("="*70)
    print("\nğŸš€ Starting Gradio interface...\n")
    
    # Initialize system on startup
    print("ğŸ”„ Initializing RAG system components...")
    success, message = state.initialize()
    print(message)
    
    if not success:
        print("\nâš ï¸  WARNING: System initialization failed!")
        print("The UI will start, but please check Ollama is running.")
        print("Run: ollama serve")
        print()
    
    # Create and launch interface
    demo = create_gradio_interface()
    
    print("\nâœ… Gradio interface ready!")
    print("="*70)
    
    # Launch with public sharing disabled by default
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
